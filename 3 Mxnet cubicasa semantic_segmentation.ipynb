{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Semantic Segmentation Algorithm\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Setup Data](#Setup-data)\n",
    "  3. [Upload to S3](#Upload-to-S3)\n",
    "4. [Training](#Training)\n",
    "5. [Hosting](#Hosting)\n",
    "6. [Inference](#Inference)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Semantic Segmentation (SS) is the task of classifying every pixel in an image with a class from a known set of labels. In contrast, [image classification](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/imageclassification_caltech) generates only one label per image and [object detection](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco) generates a bounding box along with the label for each object in the image. The semantic segmentation output is usually represented as different pixel values in the image. Therefore, the output is an integer matrix (or a grayscale image) with the same shape as the input image. This output image is also called a segmentation mask. With the Amazon SageMaker Semantic Segmentation algorithm, not only can you train your models with your own dataset but also use our pre-trained models for lazy initialization.\n",
    "\n",
    "This notebook is an end-to-end example introducing the Amazon SageMaker Semantic Segmentation algorithm. In this demo, we will demonstrate how to train and host a semantic segmentation model using the fully-convolutional network ([FCN](https://arxiv.org/abs/1605.06211)) algorithm using the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) for training. Amazon SageMaker Semantic Segmentation also provides the option of using Pyramid Scene Parsing Network([PSP](https://arxiv.org/abs/1612.01105)) and [Deeplab-v3](https://arxiv.org/abs/1706.05587) in addition to the FCN Network. Along the way, we will also demonstrate how to construct a training dataset in the format that the training job will consume. Finally, we will demonstrate how to host and validate the trained model.\n",
    "\n",
    "## Setup\n",
    "\n",
    "To train the Semantic Segmentation algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with, we need an AWS account role with SageMaker access. This role that is used to give SageMaker access to your data in S3 can automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::308537982265:role/service-role/AmazonSageMaker-ExecutionRole-20200212T095321\n",
      "CPU times: user 1.14 s, sys: 536 ms, total: 1.68 s\n",
      "Wall time: 906 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that is used to store training data and the trained model artifacts. In this notebook, we use the default bucket that comes with Sagemaker. However, you can also create a bucket and use that bucket instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-2-308537982265\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()  \n",
    "bucket_prefix = 'semantic-segmentation-cubicasa5k'\n",
    "DATA_DIR = './data/sagemaker'\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the Amazon SageMaker Semantic Segmentaion docker image, which is static and need not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825641698319.dkr.ecr.us-east-2.amazonaws.com/semantic-segmentation:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DATA_DIR}/{bucket_prefix}/\n",
    "!mkdir {DATA_DIR}/{bucket_prefix}/\n",
    "!mkdir {DATA_DIR}/{bucket_prefix}/images\n",
    "!mkdir {DATA_DIR}/{bucket_prefix}/masks\n",
    "# !rm test.jpg\n",
    "# !rm test_reshaped.jpg\n",
    "# !rm train_label_map.json\n",
    "!aws s3 rm --quiet --recursive s3://{bucket}/{bucket_prefix}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#download from previous runs \n",
    "#!aws s3 cp --quiet --recursive s3://{bucket}/floorplans_cubicasa5k/ ./data/sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data\n",
    "Move the images into appropriate directory structure as described in the [documentation](link-to-documentation). This is quite simply, moving the training images to `train` directory and so on. Fortunately, the dataset's annotations are already named in sync with the image names, satisfying one requirement of the Amazon SageMaker Semantic Segmentation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def process_files(files, target_dir,mask=False):\n",
    "    for f in files:\n",
    "        img = Image.open(f).convert(\"L\")\n",
    "        #img = img.resize((208,176), resample=0)\n",
    "        npimg = np.array(img)\n",
    "        new_fname = os.path.basename(f).split('.')[0]\n",
    "        new_fname = new_fname + \".png\" \n",
    "        imageio.imsave(os.path.join(target_dir, new_fname), npimg.astype(np.uint8))\n",
    "    return\n",
    "\n",
    "images = glob.glob(os.path.join(DATA_DIR,'images','*.jpg'))\n",
    "process_files(images, os.path.join(DATA_DIR,bucket_prefix,'images')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples found: 400\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "os.chdir(\"./\"+DATA_DIR + \"/\" + bucket_prefix)\n",
    "allfiles = glob.glob(\"images/*.png\")\n",
    "num_files = len(allfiles)\n",
    "print(\"total examples found: \" + str(num_files))\n",
    "trainratio = 0.85\n",
    "num_train = num_files * 0.85\n",
    "i = 0\n",
    "train=[]\n",
    "validation=[]\n",
    "for file in allfiles:\n",
    "    if(i<num_train):\n",
    "        train.append(file)\n",
    "    else:\n",
    "        validation.append(file)\n",
    "    i = i + 1\n",
    "with open('train.txt', 'w') as f:\n",
    "    for item in train:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('validation.txt', 'w') as f:\n",
    "    for item in validation:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping training image annotation missmatch w1:627w2:627 h1:994h2:995\n",
      "skipping training image annotation missmatch w1:1281w2:1281 h1:919h2:920\n",
      "skipping training image annotation missmatch w1:795w2:795 h1:827h2:828\n",
      "skipping training image annotation missmatch w1:978w2:979 h1:621h2:621\n",
      "skipping training image annotation missmatch w1:533w2:533 h1:509h2:510\n",
      "skipping training image annotation missmatch w1:585w2:585 h1:802h2:803\n",
      "skipping training image annotation missmatch w1:1346w2:1346 h1:936h2:937\n",
      "skipping training image annotation missmatch w1:944w2:945 h1:488h2:488\n",
      "skipping training image annotation missmatch w1:852w2:853 h1:742h2:742\n",
      "skipping training image annotation missmatch w1:836w2:837 h1:449h2:449\n",
      "skipping training image annotation missmatch w1:836w2:837 h1:901h2:901\n",
      "skipping training image annotation missmatch w1:459w2:460 h1:579h2:579\n",
      "skipping training image annotation missmatch w1:2005w2:2006 h1:874h2:874\n",
      "skipping training image annotation missmatch w1:404w2:404 h1:451h2:452\n",
      "skipping training image annotation missmatch w1:537w2:537 h1:861h2:862\n",
      "skipping training image annotation missmatch w1:774w2:774 h1:488h2:489\n",
      "skipping training image annotation missmatch w1:886w2:887 h1:484h2:485\n",
      "skipping training image annotation missmatch w1:480w2:481 h1:565h2:565\n",
      "skipping training image annotation missmatch w1:1337w2:1337 h1:936h2:937\n",
      "skipping training image annotation missmatch w1:438w2:439 h1:894h2:895\n",
      "skipping training image annotation missmatch w1:754w2:754 h1:802h2:803\n",
      "skipping training image annotation missmatch w1:484w2:485 h1:911h2:912\n",
      "skipping training image annotation missmatch w1:459w2:460 h1:484h2:484\n",
      "skipping training image annotation missmatch w1:780w2:780 h1:977h2:978\n",
      "skipping training image annotation missmatch w1:811w2:812 h1:516h2:516\n",
      "skipping training image annotation missmatch w1:994w2:995 h1:601h2:601\n",
      "skipping training image annotation missmatch w1:476w2:477 h1:537h2:537\n",
      "skipping training image annotation missmatch w1:449w2:449 h1:969h2:970\n",
      "skipping training image annotation missmatch w1:682w2:682 h1:877h2:878\n",
      "skipping training image annotation missmatch w1:204w2:205 h1:437h2:437\n",
      "skipping training image annotation missmatch w1:451w2:452 h1:1002h2:1002\n",
      "skipping training image annotation missmatch w1:401w2:402 h1:600h2:600\n",
      "skipping training image annotation missmatch w1:886w2:886 h1:463h2:464\n",
      "skipping training image annotation missmatch w1:549w2:549 h1:401h2:402\n",
      "skipping training image annotation missmatch w1:1391w2:1391 h1:878h2:879\n",
      "skipping training image annotation missmatch w1:1291w2:1291 h1:961h2:962\n",
      "skipping validation image annotation missmatch w1:1055w2:1055 h1:936h2:937\n",
      "skipping validation image annotation missmatch w1:1011w2:1012 h1:321h2:321\n",
      "skipping validation image annotation missmatch w1:1742w2:1742 h1:1880h2:1881\n",
      "skipping validation image annotation missmatch w1:815w2:815 h1:509h2:510\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "PREFIX = DATA_DIR + \"/\" + bucket_prefix\n",
    "\n",
    "# Create directory structure mimicing the s3 bucket where data is to be dumped.\n",
    "os.makedirs(PREFIX+'/train', exist_ok=True)\n",
    "os.makedirs(PREFIX+'/validation', exist_ok=True)\n",
    "os.makedirs(PREFIX+'/train_annotation', exist_ok=True)\n",
    "os.makedirs(PREFIX+'/validation_annotation', exist_ok=True)\n",
    "\n",
    "# Create a list of all training images.\n",
    "filename = PREFIX+'/train.txt'\n",
    "with open(filename) as f:\n",
    "    train_list = f.read().splitlines() \n",
    "\n",
    "# Create a list of all validation images.\n",
    "filename = PREFIX+'/validation.txt'\n",
    "with open(filename) as f:\n",
    "    val_list = f.read().splitlines() \n",
    "\n",
    "# Move the jpg images in training list to train directory and png images to train_annotation directory.\n",
    "for i in train_list:\n",
    "    im1 = Image.open(PREFIX+'/'+i)\n",
    "    width1, height1 = im1.size\n",
    "    im2 = Image.open(DATA_DIR+'/'+i.replace('images', 'masks'))\n",
    "    width2, height2 = im2.size\n",
    "    if(width1!=width2 or height1!=height2): \n",
    "        print('skipping training image annotation missmatch ' + 'w1:' + str(width1) +  'w2:' + str(width2) +' h1:' + str(height1) + 'h2:' + str(height2))\n",
    "    else:\n",
    "        shutil.copy2(PREFIX+'/'+i, PREFIX +'/train/'+i.replace('images', ''))\n",
    "        shutil.copy2(DATA_DIR+'/'+i.replace('images', 'masks'),PREFIX+'/train_annotation/'+i.replace('images', '').replace('jpg', 'png'), )\n",
    "\n",
    "# Move the jpg images in validation list to validation directory and png images to validation_annotation directory.\n",
    "for i in val_list:\n",
    "    im1 = Image.open(PREFIX+'/'+i)\n",
    "    width1, height1 = im1.size\n",
    "    im2 = Image.open(DATA_DIR+'/'+i.replace('images', 'masks'))\n",
    "    width2, height2 = im2.size\n",
    "    if(width1!=width2 or height1!=height2): \n",
    "        print('skipping validation image annotation missmatch ' + 'w1:' + str(width1) +  'w2:' + str(width2) +' h1:' + str(height1) + 'h2:' + str(height2))\n",
    "    else:\n",
    "        shutil.copy2(PREFIX+'/'+i, PREFIX+'/validation/'+i.replace('images', ''))\n",
    "        shutil.copy2(DATA_DIR+'/'+i.replace('images', 'masks'),PREFIX+'/validation_annotation/'+i.replace('images', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "def process_masks(files):\n",
    "    for f in files:\n",
    "        #print(f)\n",
    "        img = Image.open(f)\n",
    "        #print(f)\n",
    "        # Load image and ensure it is 3-channel RGB...\n",
    "        # ... not 1-channel greyscale, not 4-channel RGBA, not 1-channel palette\n",
    "        im = img.convert('RGB')\n",
    "        # Make into Numpy array of RGB and get dimensions\n",
    "        RGB = np.array(im)\n",
    "        h, w = RGB.shape[:2]\n",
    "\n",
    "        # Add an alpha channel, fully opaque (255)\n",
    "        RGBA = np.dstack((RGB, np.zeros((h,w),dtype=np.uint8)+255))\n",
    "\n",
    "        # Make mask of black pixels - mask is True where image is black\n",
    "        mBlack = (RGBA[:, :, 0:3] != [0,0,0]).all(2)\n",
    "\n",
    "        # Make all pixels matched by mask into transparent ones\n",
    "        RGBA[mBlack] = (0,0,0,0)\n",
    "\n",
    "        # Convert Numnpy array back to PIL Image and save\n",
    "        \n",
    "        #imageio.imsave(f.replace('_old',''), RGBA.astype(np.uint8))\n",
    "        maskimage = Image.fromarray(RGBA.astype(np.uint8))\n",
    "        p_img = maskimage.convert(\"L\")\n",
    "        p_img.save(f,transparency=255)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "images = glob(PREFIX + '/train_annotation/*.png')\n",
    "process_masks(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob(PREFIX +'/validation_annotation/*.png')\n",
    "process_masks(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the move was completed correctly. If it was done correctly, the number of jpeg images in `train` and png images in `train_annotation` must be the same, and so in validation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num Train Images = 304\n",
      " Num Train annos = 304\n",
      " Num Validation Images = 56\n",
      " Num Validation annos = 56\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "num_training_samples=len(glob.glob1(PREFIX+'/train',\"*.png\"))\n",
    "print ( ' Num Train Images = ' + str(num_training_samples))\n",
    "print ( ' Num Train annos = ' + str(len(glob.glob1(PREFIX+'/train_annotation',\"*.png\"))))\n",
    "\n",
    "assert num_training_samples == len(glob.glob1(PREFIX+'/train_annotation',\"*.png\"))\n",
    "\n",
    "print ( ' Num Validation Images = ' + str(len(glob.glob1(PREFIX+'/validation',\"*.png\"))))\n",
    "\n",
    "print ( ' Num Validation annos = ' + str(len(glob.glob1(PREFIX+'/validation_annotation',\"*.png\"))))\n",
    "assert len(glob.glob1(PREFIX+'/validation',\"*.png\")) == len(glob.glob1(PREFIX+'/validation_annotation',\"*.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now move our prepared datset to the S3 bucket that we decided to use in this notebook earlier. Notice the following directory structure that is used.\n",
    "\n",
    "```bash\n",
    "root \n",
    "|-train/\n",
    "|-train_annotation/\n",
    "|-validation/\n",
    "|-validation_annotation/\n",
    "\n",
    "```\n",
    "Notice also that all the images in the `_annotation` directory are all indexed PNG files. This implies that the metadata (color mapping modes) of the files contain information on how to map the indices to colors and vice versa. Having an indexed PNG is an advantage as the images will be rendered by image viewers as color images, but the image themsevels only contain integers. The integers are also within `[0, 1 ... c-1, 255]`  for a `c` class segmentation problem, with `255` as 'hole' or 'ignore' class. We allow any mode that is a [recognized standard](https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html#concept-modes) as long as they are read as integers.\n",
    "\n",
    "While we recommend the format with default color mapping modes such as PASCAL, we also allow the customers to specify their own label maps. Refer to the [documentation](Permalink-to-label-map-documentation-section) for more details. The label map for the PASCAL VOC dataset, is the default (which we use incase no label maps are provided): \n",
    "```json\n",
    "{\n",
    "    \"scale\": 1\n",
    "}```\n",
    "This essentially tells us to simply use the images as read as integers as labels directly. Since we are using PASCAL dataset, let us create (recreate the default just for demonstration) label map for training channel and let the algorithm use the default (which is exactly the same for the validation channel). If `label_map` is used, please pass it to the label_map channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = { \"scale\": 1 }\n",
    "with open('train_label_map.json', 'w') as lm_fname:\n",
    "    json.dump(label_map, lm_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic-segmentation-cubicasa5k/train_annotation\n"
     ]
    }
   ],
   "source": [
    "# Create channel names for the s3 bucket.\n",
    "train_channel = bucket_prefix + '/train'\n",
    "validation_channel = bucket_prefix + '/validation'\n",
    "train_annotation_channel = bucket_prefix + '/train_annotation'\n",
    "validation_annotation_channel = bucket_prefix + '/validation_annotation'\n",
    "print(train_annotation_channel)\n",
    "# label_map_channel = bucket_prefix + '/label_map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "Let us now upload our dataset including our label map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.42 s, sys: 313 ms, total: 4.74 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# upload the appropraite directory up to s3 respectively for all directories.\n",
    "sess.upload_data(path=PREFIX+'/train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=PREFIX+'/validation', bucket=bucket, key_prefix=validation_channel)\n",
    "sess.upload_data(path=PREFIX+'/train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\n",
    "sess.upload_data(path=PREFIX+'/validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "# sess.upload_data(path='train_label_map.json', bucket=bucket, key_prefix=label_map_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job. Let us use another channel in the same S3 bucket for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-308537982265/semantic-segmentation-cubicasa5k/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, bucket_prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our segmentation algorithm. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job. Let us name our training job as `ss-notebook-demo`. Let us also use a nice-and-fast GPU instance (`ml.p3.2xlarge`) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.8xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'ss-floorplans-demo-estimator-cubicasa8x',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic segmentation algorithm at its core has two compoenents.\n",
    "\n",
    "- An encoder or backbone network,\n",
    "- A decoder or algorithm network. \n",
    "\n",
    "The encoder or backbone network is typically a regular convolutional neural network that may or maynot have had their layers pre-trained on an alternate task such as the [classification task of ImageNet images](http://www.image-net.org/). The Amazon SageMaker Semantic Segmentation algorithm comes with two choices of pre-trained or to be trained-from-scratch backbone networks ([ResNets](https://arxiv.org/abs/1512.03385) 50 or 101). \n",
    "\n",
    "The decoder is a network that picks up the outputs of one or many layers from the backbone and reconstructs the segmentation mask from it. Amazon SageMaker Semantic Segmentation algorithm comes with a choice of the [Fully-convolutional network (FCN)](https://arxiv.org/abs/1605.06211) or the [Pyramid scene parsing (PSP) network](https://arxiv.org/abs/1612.01105).\n",
    "\n",
    "The algorithm also has ample options for hyperparameters that help configure the training job. The next step in our training, is to setup these networks and hyperparameters along with data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Semantic Segmentation [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. For the sake of demonstration let us run only `10` epochs. Based on our tests, train the model for `30` epochs with similar settings should give us 'reasonable' segmentation results on the Pascal VOC data. For the most part, we will stick to using the simplest of settings. For more information on the hyperparameters of this algorithm, refer to the [documentation](perma-link-to-hyperparameter-section-in-documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-50', # This is the encoder. Other option is resnet-101\n",
    "                             algorithm='fcn', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model=True, # Use the pre-trained model.\n",
    "                             crop_size=600, # Size of image random crop.                             \n",
    "                             num_classes=2, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=100, # Number of epochs to run.\n",
    "                             learning_rate=0.001,                             \n",
    "                             optimizer='rmsprop', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=16, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=8,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=2, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=50, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm uses to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyperparameters for this object and we have our data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the `Estimator` classes are provisioned and are setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data and the availability of the type of instances. Therefore it might be a few minutes before we start getting data logs for our training jobs. The data logs will also print out training loss on the training data, which is the pixel-wise cross-entropy loss as described in the algorithm papers. The data logs will also print out pixel-wise label accuracy and mean intersection-over-union (mIoU) on the validation data after a run of the dataset once or one epoch. These metrics measure the quality of the model under training.\n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01 07:35:06 Starting - Starting the training job...\n",
      "2020-04-01 07:35:08 Starting - Launching requested ML instances...\n",
      "2020-04-01 07:36:04 Starting - Preparing the instances for training...............\n",
      "2020-04-01 07:38:28 Downloading - Downloading input data...\n",
      "2020-04-01 07:38:39 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:48 INFO 140322268817216] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'syncbn': u'False', u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'5', u'epochs': u'10', u'_workers': u'16', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'240', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'early_stopping_patience': u'4', u'momentum': u'0.9', u'optimizer': u'sgd', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'16', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'early_stopping': u'False', u'algorithm': u'fcn', u'_logging_frequency': u'20', u'num_training_samples': u'8', u'_kvstore': u'device', u'precision_dtype': u'float32'}\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:48 INFO 140322268817216] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'optimizer': u'rmsprop', u'algorithm': u'fcn', u'lr_scheduler': u'poly', u'use_pretrained_model': u'True', u'backbone': u'resnet-50', u'early_stopping_min_epochs': u'50', u'epochs': u'100', u'validation_mini_batch_size': u'8', u'num_training_samples': u'304', u'num_classes': u'2', u'mini_batch_size': u'16', u'early_stopping_patience': u'2', u'early_stopping': u'True', u'crop_size': u'600'}\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:48 INFO 140322268817216] Final configuration: {u'syncbn': u'False', u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'50', u'epochs': u'100', u'_workers': u'16', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'600', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'num_classes': u'2', u'early_stopping_patience': u'2', u'momentum': u'0.9', u'optimizer': u'rmsprop', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'8', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'early_stopping': u'True', u'algorithm': u'fcn', u'_logging_frequency': u'20', u'num_training_samples': u'304', u'_kvstore': u'device', u'precision_dtype': u'float32'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:48 INFO 140322268817216] Using default worker.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:48 INFO 140322268817216] font search path ['/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:49 INFO 140322268817216] generated new fontManager\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:49 INFO 140322268817216] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:49 INFO 140322268817216] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:49 INFO 140322268817216] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:49 INFO 140322268817216] Loaded iterator creator application/json for content type ('application/json', '1.0')\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 WARNING 140322268817216] /opt/ml/input/data/train/train_annotation is not a readable image file\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 WARNING 140322268817216] label maps not provided, using defaults.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] #label_map train :{'scale': 1}\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 WARNING 140322268817216] /opt/ml/input/data/validation/validation_annotation is not a readable image file\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 WARNING 140322268817216] label maps not provided, using defaults.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] #label_map validation :{'scale': 1}\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] nvidia-smi took: 0.0755531787872 secs to identify 4 gpus\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] Number of GPUs being used: 4\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:39:50 INFO 140322268817216] Number of GPUs being used: 4\u001b[0m\n",
      "\u001b[34m[07:39:52] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1160.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n",
      "\u001b[34m[07:39:53] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1160.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n",
      "\u001b[34m[07:39:54] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1160.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n",
      "\u001b[34m[07:39:54] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1160.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n",
      "\n",
      "2020-04-01 07:39:45 Training - Training image download completed. Training in progress.\u001b[34m[04/01/2020 07:40:03 INFO 140322268817216] LRScheduler setup: iters per epoch: 19, num_epochs 100\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1585726803.715182, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1585726803.715134}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:24 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 0, train loss: 0.07690435607831828 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:24 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 0, train throughput: 15.7158740188 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 0, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 0, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 0, validation throughput: 43.0213154963 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:27 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:27 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:27 INFO 140322268817216] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1585726827.530047, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 0}, \"StartTime\": 1585726803.715406}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:47 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 1, train loss: 0.0005933736128530367 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:47 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 1, train throughput: 16.4765346355 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 1, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 1, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 1, validation throughput: 43.7177788457 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:49 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:40:50 INFO 140322268817216] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1585726850.110584, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 1}, \"StartTime\": 1585726827.530183}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:10 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 2, train loss: 0.00044326920224672283 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:10 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 2, train throughput: 16.4766306589 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 2, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 2, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:12 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 2, validation throughput: 43.8858099492 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:12 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:41:12 INFO 140322268817216] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1585726872.598985, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 2}, \"StartTime\": 1585726850.11072}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 13, train loss: 0.000527524296270904 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 13, train throughput: 15.0141270173 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:28 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 13, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:28 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 13, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:28 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 13, validation throughput: 43.6188691642 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:28 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:28 INFO 140322268817216] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1585727128.747833, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 13}, \"StartTime\": 1585727104.430612}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 14, train loss: 0.0005198236080774988 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:50 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 14, train throughput: 15.0377038812 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:52 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 14, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:52 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 14, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:52 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 14, validation throughput: 43.8691216184 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:52 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:45:52 INFO 140322268817216] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1585727152.76475, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 14}, \"StartTime\": 1585727128.747972}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 15, train loss: 0.0005185807236366122 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 15, train throughput: 14.9843881747 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 15, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 15, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:16 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 15, validation throughput: 43.616212429 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:16 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:16 INFO 140322268817216] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1585727176.997666, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 15}, \"StartTime\": 1585727152.764955}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 16, train loss: 0.0005224039877679101 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 16, train throughput: 14.9085692594 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 16, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 16, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:41 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 16, validation throughput: 44.0517831254 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:41 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:46:41 INFO 140322268817216] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1585727201.391733, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 16}, \"StartTime\": 1585727176.997802}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 17, train loss: 0.0005147813120090991 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 17, train throughput: 15.2039143436 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 17, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 17, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:05 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 17, validation throughput: 44.1095929308 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:05 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:05 INFO 140322268817216] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1585727225.307073, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 17}, \"StartTime\": 1585727201.391873}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 18, train loss: 0.0005076837398136329 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 18, train throughput: 15.0343970434 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 18, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 18, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:29 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 18, validation throughput: 43.687342983 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:29 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:29 INFO 140322268817216] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1585727249.715936, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 18}, \"StartTime\": 1585727225.307208}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 19, train loss: 0.0005072815601076735 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 19, train throughput: 15.1386222807 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 19, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 19, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:53 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 19, validation throughput: 43.0068529056 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:53 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:47:53 INFO 140322268817216] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1585727273.970999, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 19}, \"StartTime\": 1585727249.716073}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 20, train loss: 0.0004952556501896316 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:16 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 20, train throughput: 14.8978660729 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:18 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 20, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:18 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 20, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:18 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 20, validation throughput: 44.2905164627 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:18 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:18 INFO 140322268817216] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1585727298.380057, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 20}, \"StartTime\": 1585727273.971138}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 21, train loss: 0.0004901209988594545 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 21, train throughput: 15.2119792001 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 21, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 21, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:41 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 21, validation throughput: 42.9241166768 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:41 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:48:42 INFO 140322268817216] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1585727322.196883, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 21}, \"StartTime\": 1585727298.380236}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 22, train loss: 0.0004843449117432005 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 22, train throughput: 15.3334795247 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 22, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 22, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:05 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 22, validation throughput: 44.0244285031 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:05 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:05 INFO 140322268817216] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 23, \"sum\": 23.0, \"min\": 23}}, \"EndTime\": 1585727345.935926, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 22}, \"StartTime\": 1585727322.197021}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 23, train loss: 0.00047638159594498575 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 23, train throughput: 15.3353236914 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 23, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 23, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:29 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 23, validation throughput: 43.3179997234 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:29 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:29 INFO 140322268817216] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1585727369.774567, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 23}, \"StartTime\": 1585727345.936062}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 24, train loss: 0.0004668665512183093 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 24, train throughput: 15.1588290888 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 24, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 24, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:53 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 24, validation throughput: 44.2168687899 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:53 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:49:53 INFO 140322268817216] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}}, \"EndTime\": 1585727393.987092, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 24}, \"StartTime\": 1585727369.774745}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:15 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 25, train loss: 0.00046323079693952183 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:15 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 25, train throughput: 15.2702996333 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 25, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 25, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:17 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 25, validation throughput: 43.8894181333 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:17 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:17 INFO 140322268817216] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1585727417.826988, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 25}, \"StartTime\": 1585727393.987228}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 26, train loss: 0.00045838649844211573 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 26, train throughput: 15.1750964108 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 26, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 26, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:41 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 26, validation throughput: 44.0289095919 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:41 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:50:41 INFO 140322268817216] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 27, \"sum\": 27.0, \"min\": 27}}, \"EndTime\": 1585727441.959076, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 26}, \"StartTime\": 1585727417.827128}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 27, train loss: 0.00045803645212129737 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 27, train throughput: 15.4741530461 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 27, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 27, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:05 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 27, validation throughput: 44.1853851757 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:05 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:05 INFO 140322268817216] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1585727465.655061, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 27}, \"StartTime\": 1585727441.959263}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 28, train loss: 0.00044996382271883224 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 28, train throughput: 15.5777105601 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:28 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 28, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:28 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 28, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:28 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 28, validation throughput: 43.8120707623 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:28 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:29 INFO 140322268817216] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 29, \"sum\": 29.0, \"min\": 29}}, \"EndTime\": 1585727489.11817, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 28}, \"StartTime\": 1585727465.655194}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 29, train loss: 0.00044599251850740984 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:50 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 29, train throughput: 15.3720474353 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:52 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 29, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:52 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 29, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:52 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 29, validation throughput: 43.7267721618 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:52 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:51:52 INFO 140322268817216] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1585727512.783727, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 29}, \"StartTime\": 1585727489.118308}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 30, train loss: 0.000443796614203357 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 30, train throughput: 15.329366787 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 30, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 30, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:16 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 30, validation throughput: 43.4222178511 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:16 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:16 INFO 140322268817216] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 31, \"sum\": 31.0, \"min\": 31}}, \"EndTime\": 1585727536.754176, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 30}, \"StartTime\": 1585727512.783862}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 31, train loss: 0.00044169181905760383 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:38 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 31, train throughput: 15.3751540809 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:40 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 31, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:40 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 31, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:40 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 31, validation throughput: 43.4126031294 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:40 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:52:40 INFO 140322268817216] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1585727560.3277, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 31}, \"StartTime\": 1585727536.754311}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 32, train loss: 0.00043947027695953454 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:01 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 32, train throughput: 15.6088509898 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 32, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 32, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 32, validation throughput: 43.4925393807 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:03 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:03 INFO 140322268817216] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 33, \"sum\": 33.0, \"min\": 33}}, \"EndTime\": 1585727583.919089, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 32}, \"StartTime\": 1585727560.327837}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 33, train loss: 0.0004356628449290599 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:25 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 33, train throughput: 15.5086749535 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 33, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 33, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 33, validation throughput: 43.5170598248 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:27 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:27 INFO 140322268817216] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1585727607.586808, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 33}, \"StartTime\": 1585727583.919228}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 34, train loss: 0.0004372776173496325 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 34, train throughput: 15.2966843402 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 34, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 34, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 34, validation throughput: 43.9759211912 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:51 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:53:51 INFO 140322268817216] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 35, \"sum\": 35.0, \"min\": 35}}, \"EndTime\": 1585727631.314753, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 34}, \"StartTime\": 1585727607.586949}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 35, train loss: 0.0004301685192714151 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:12 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 35, train throughput: 15.3606799643 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 35, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 35, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 35, validation throughput: 43.905810036 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:14 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:15 INFO 140322268817216] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1585727655.047451, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 35}, \"StartTime\": 1585727631.31489}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:36 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 36, train loss: 0.0004319635998782408 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:36 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 36, train throughput: 15.4299831591 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 36, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 36, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:38 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 36, validation throughput: 44.5792498617 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:38 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:54:38 INFO 140322268817216] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 37, \"sum\": 37.0, \"min\": 37}}, \"EndTime\": 1585727678.69757, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 36}, \"StartTime\": 1585727655.047598}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:00 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 37, train loss: 0.00043089440250217815 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:00 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 37, train throughput: 15.3299722219 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 37, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 37, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:02 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 37, validation throughput: 42.4691954122 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:02 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:02 INFO 140322268817216] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1585727702.558275, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 37}, \"StartTime\": 1585727678.697796}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:23 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 38, train loss: 0.00042925489986756524 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:23 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 38, train throughput: 15.5243733342 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 38, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 38, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 38, validation throughput: 41.6437700213 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:26 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:26 INFO 140322268817216] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 39, \"sum\": 39.0, \"min\": 39}}, \"EndTime\": 1585727726.248284, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 38}, \"StartTime\": 1585727702.558414}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:47 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 39, train loss: 0.00042858520426211486 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:47 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 39, train throughput: 15.5908850428 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 39, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 39, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 39, validation throughput: 43.732984186 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:49 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:55:49 INFO 140322268817216] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1585727749.724408, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 39}, \"StartTime\": 1585727726.248431}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:10 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 40, train loss: 0.0004284914259762062 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:10 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 40, train throughput: 15.8331997952 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 40, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 40, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:12 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 40, validation throughput: 43.5987654 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:12 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:12 INFO 140322268817216] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 41, \"sum\": 41.0, \"min\": 41}}, \"EndTime\": 1585727772.879308, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 40}, \"StartTime\": 1585727749.724646}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:33 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 41, train loss: 0.0004278097947367075 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:33 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 41, train throughput: 15.9835058068 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:35 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 41, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:35 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 41, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:35 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 41, validation throughput: 43.6851247665 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:35 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:35 INFO 140322268817216] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1585727795.901954, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 41}, \"StartTime\": 1585727772.879461}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:57 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 42, train loss: 0.00042738749202965806 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:57 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 42, train throughput: 15.468259706 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:59 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 42, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:59 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 42, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:59 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 42, validation throughput: 43.7272198895 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:59 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:56:59 INFO 140322268817216] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 43, \"sum\": 43.0, \"min\": 43}}, \"EndTime\": 1585727819.487134, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 42}, \"StartTime\": 1585727795.902096}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:20 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 43, train loss: 0.0004269241793448792 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:20 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 43, train throughput: 15.5037858274 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:22 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 43, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:22 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 43, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:22 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 43, validation throughput: 43.907525412 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:22 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:23 INFO 140322268817216] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1585727843.01018, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 43}, \"StartTime\": 1585727819.487268}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:43 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 44, train loss: 0.00042658377711440583 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:43 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 44, train throughput: 15.9481730947 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:45 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 44, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:45 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 44, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:45 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 44, validation throughput: 43.3021874072 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:45 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:57:46 INFO 140322268817216] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}}, \"EndTime\": 1585727866.122609, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 44}, \"StartTime\": 1585727843.010313}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:06 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 45, train loss: 0.0004262871313441888 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:06 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 45, train throughput: 15.9070562772 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:08 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 45, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:08 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 45, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:08 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 45, validation throughput: 43.9603819266 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:08 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:09 INFO 140322268817216] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1585727889.154503, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 45}, \"StartTime\": 1585727866.122749}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:30 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 46, train loss: 0.0004260343385364027 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:30 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 46, train throughput: 15.6964862771 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:32 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 46, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:32 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 46, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:32 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 46, validation throughput: 43.7163874504 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:32 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:32 INFO 140322268817216] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 47, \"sum\": 47.0, \"min\": 47}}, \"EndTime\": 1585727912.664174, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 46}, \"StartTime\": 1585727889.154642}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 47, train loss: 0.0004258355122721313 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:53 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 47, train throughput: 15.6907075854 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:55 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 47, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:55 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 47, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:55 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 47, validation throughput: 43.8367974791 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:55 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:58:56 INFO 140322268817216] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1585727936.065774, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 47}, \"StartTime\": 1585727912.664354}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 48, train loss: 0.0004256999473957541 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:17 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 48, train throughput: 15.6206579627 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:19 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 48, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:19 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 48, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:19 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 48, validation throughput: 43.9775185389 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:19 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:19 INFO 140322268817216] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 49, \"sum\": 49.0, \"min\": 49}}, \"EndTime\": 1585727959.436876, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 48}, \"StartTime\": 1585727936.065915}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:40 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 49, train loss: 0.00042562797515481517 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:40 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 49, train throughput: 15.4699737067 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:42 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 49, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:42 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 49, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:42 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 49, validation throughput: 44.2279007392 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:42 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 07:59:43 INFO 140322268817216] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1585727983.035183, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 49}, \"StartTime\": 1585727959.437013}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:04 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 50, train loss: 0.00042545181579618275 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:04 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 50, train throughput: 15.2268316245 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:06 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 50, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:06 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 50, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:06 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 50, validation throughput: 44.1010210897 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:06 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:06 INFO 140322268817216] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 51, \"sum\": 51.0, \"min\": 51}}, \"EndTime\": 1585728006.889705, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 50}, \"StartTime\": 1585727983.035332}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 51, train loss: 0.0004254491746235688 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 51, train throughput: 15.7153827952 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 51, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 51, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:29 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 51, validation throughput: 43.9532003752 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:29 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:30 INFO 140322268817216] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1585728030.088689, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 51}, \"StartTime\": 1585728006.889843}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 52, train loss: 0.00042531726495852987 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 52, train throughput: 15.5273786762 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 52, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 52, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:53 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 52, validation throughput: 43.1528930023 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:53 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:00:53 INFO 140322268817216] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 53, \"sum\": 53.0, \"min\": 53}}, \"EndTime\": 1585728053.710709, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 52}, \"StartTime\": 1585728030.088828}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:15 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 53, train loss: 0.00042526949025494486 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:15 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 53, train throughput: 15.3428425214 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 53, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 53, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:17 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 53, validation throughput: 43.902675102 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:17 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:17 INFO 140322268817216] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1585728077.384081, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 53}, \"StartTime\": 1585728053.710846}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 54, train loss: 0.0004251424099110361 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 54, train throughput: 15.0031635679 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 54, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:41 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 54, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:41 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 54, validation throughput: 44.0202453263 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:41 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:01:41 INFO 140322268817216] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 55, \"sum\": 55.0, \"min\": 55}}, \"EndTime\": 1585728101.599666, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 54}, \"StartTime\": 1585728077.384253}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 55, train loss: 0.00042519635386078765 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 55, train throughput: 15.0160093986 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 55, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:05 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 55, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:05 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 55, validation throughput: 43.9443521146 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:05 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:06 INFO 140322268817216] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1585728126.076013, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 55}, \"StartTime\": 1585728101.5998}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 56, train loss: 0.00042509477421653603 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 56, train throughput: 15.3522025418 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 56, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:29 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 56, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:29 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 56, validation throughput: 44.3794311454 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:29 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:29 INFO 140322268817216] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 57, \"sum\": 57.0, \"min\": 57}}, \"EndTime\": 1585728149.824508, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 56}, \"StartTime\": 1585728126.076151}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 57, train loss: 0.0004251062196808083 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 57, train throughput: 15.3852786638 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 57, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:53 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 57, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:53 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 57, validation throughput: 43.9070657766 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:53 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:02:53 INFO 140322268817216] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1585728173.714856, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 57}, \"StartTime\": 1585728149.824661}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:15 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 58, train loss: 0.00042505928286118433 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:15 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 58, train throughput: 15.4219032276 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 58, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:17 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 58, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:17 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 58, validation throughput: 43.6018327986 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:17 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:17 INFO 140322268817216] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 59, \"sum\": 59.0, \"min\": 59}}, \"EndTime\": 1585728197.441615, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 58}, \"StartTime\": 1585728173.71499}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 59, train loss: 0.0004250632884673235 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:38 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 59, train throughput: 15.4743519219 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:40 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 59, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:40 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 59, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:40 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 59, validation throughput: 44.1724387601 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:40 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:03:40 INFO 140322268817216] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1585728220.975103, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 59}, \"StartTime\": 1585728197.441757}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 60, train loss: 0.0004250560485065523 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:02 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 60, train throughput: 15.4005945446 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:04 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 60, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:04 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 60, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:04 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 60, validation throughput: 44.1161710756 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:04 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:04 INFO 140322268817216] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 61, \"sum\": 61.0, \"min\": 61}}, \"EndTime\": 1585728244.511477, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 60}, \"StartTime\": 1585728220.97524}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 61, train loss: 0.0004250097056348367 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 61, train throughput: 15.214339219 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:28 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 61, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:28 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 61, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:28 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 61, validation throughput: 44.206616095 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:28 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:28 INFO 140322268817216] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1585728268.493051, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 61}, \"StartTime\": 1585728244.511615}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 62, train loss: 0.0004249905832693912 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:50 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 62, train throughput: 14.9866453353 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:52 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 62, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:52 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 62, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:52 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 62, validation throughput: 43.5612550587 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:52 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:04:52 INFO 140322268817216] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 63, \"sum\": 63.0, \"min\": 63}}, \"EndTime\": 1585728292.666546, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 62}, \"StartTime\": 1585728268.493188}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 63, train loss: 0.0004250282524337404 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 63, train throughput: 15.4349059961 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 63, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:16 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 63, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:16 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 63, validation throughput: 43.4554607521 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:16 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:16 INFO 140322268817216] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1585728316.305734, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 63}, \"StartTime\": 1585728292.666687}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 64, train loss: 0.0004250279265474283 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:37 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 64, train throughput: 15.3801870637 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 64, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 64, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 64, validation throughput: 43.7796837282 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:39 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:05:40 INFO 140322268817216] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 65, \"sum\": 65.0, \"min\": 65}}, \"EndTime\": 1585728340.142869, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 64}, \"StartTime\": 1585728316.305894}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 65, train loss: 0.00042499218512890175 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:01 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 65, train throughput: 15.2146912335 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 65, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 65, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 65, validation throughput: 43.5444896939 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:03 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:04 INFO 140322268817216] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1585728364.018615, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 65}, \"StartTime\": 1585728340.143009}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 66, train loss: 0.0004249617942198376 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:25 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 66, train throughput: 15.1646875314 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 66, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 66, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 66, validation throughput: 43.3811563185 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:27 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:28 INFO 140322268817216] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 67, \"sum\": 67.0, \"min\": 67}}, \"EndTime\": 1585728388.212245, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 66}, \"StartTime\": 1585728364.018751}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 67, train loss: 0.00042493439296346257 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 67, train throughput: 15.201207419 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 67, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 67, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 67, validation throughput: 43.6160747412 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:51 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:06:51 INFO 140322268817216] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1585728411.962177, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 67}, \"StartTime\": 1585728388.212381}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 68, train loss: 0.00042495863568834275 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:13 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 68, train throughput: 15.1957001169 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:15 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 68, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:15 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 68, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:15 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 68, validation throughput: 43.565739316 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:15 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:15 INFO 140322268817216] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 69, \"sum\": 69.0, \"min\": 69}}, \"EndTime\": 1585728435.943167, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 68}, \"StartTime\": 1585728411.962316}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 69, train loss: 0.00042492842284876754 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:37 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 69, train throughput: 15.1063893139 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 69, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 69, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 69, validation throughput: 43.3832956939 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:39 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:07:40 INFO 140322268817216] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1585728460.044653, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 69}, \"StartTime\": 1585728435.943306}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 70, train loss: 0.00042501734385855104 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:01 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 70, train throughput: 15.1881141885 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 70, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:03 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 70, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:03 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 70, validation throughput: 43.2122533285 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:03 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:04 INFO 140322268817216] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}}, \"EndTime\": 1585728484.097436, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 70}, \"StartTime\": 1585728460.0448}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 71, train loss: 0.0004248891901194216 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:25 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 71, train throughput: 15.6249432523 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 71, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:27 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 71, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:27 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 71, validation throughput: 43.5460558528 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:27 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:27 INFO 140322268817216] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1585728507.687027, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 71}, \"StartTime\": 1585728484.097609}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 72, train loss: 0.0004249259865518022 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 72, train throughput: 15.444445688 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 72, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:51 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 72, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:51 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 72, validation throughput: 43.1866139731 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:51 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:08:51 INFO 140322268817216] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 73, \"sum\": 73.0, \"min\": 73}}, \"EndTime\": 1585728531.326884, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 72}, \"StartTime\": 1585728507.687171}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 73, train loss: 0.0004249596907021968 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:13 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 73, train throughput: 15.3033914721 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 73, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 73, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 73, validation throughput: 43.6626059703 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:15 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:15 INFO 140322268817216] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1585728555.24208, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 73}, \"StartTime\": 1585728531.327022}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 74, train loss: 0.0004249764200434775 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:37 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 74, train throughput: 15.2465006376 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 74, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:39 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 74, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:39 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 74, validation throughput: 44.0220769153 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:39 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:09:39 INFO 140322268817216] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 75, \"sum\": 75.0, \"min\": 75}}, \"EndTime\": 1585728579.305179, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 74}, \"StartTime\": 1585728555.242222}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:00 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 75, train loss: 0.0004249298669348814 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:00 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 75, train throughput: 15.3650429997 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 75, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 75, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:02 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 75, validation throughput: 43.9241197683 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:02 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:03 INFO 140322268817216] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1585728603.127225, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 75}, \"StartTime\": 1585728579.305319}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:24 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 76, train loss: 0.0004248811509521482 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:24 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 76, train throughput: 15.3026149482 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 76, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 76, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 76, validation throughput: 43.2075712992 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:26 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:26 INFO 140322268817216] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 77, \"sum\": 77.0, \"min\": 77}}, \"EndTime\": 1585728626.978152, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 76}, \"StartTime\": 1585728603.127362}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:48 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 77, train loss: 0.00042489135069588785 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:48 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 77, train throughput: 15.3103671897 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 77, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 77, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:50 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 77, validation throughput: 43.4557180245 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:50 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:10:50 INFO 140322268817216] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1585728650.812311, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 77}, \"StartTime\": 1585728626.978288}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 78, train loss: 0.0004248684498107698 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:12 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 78, train throughput: 15.3380987071 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 78, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 78, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 78, validation throughput: 44.0185376306 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:14 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:14 INFO 140322268817216] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 79, \"sum\": 79.0, \"min\": 79}}, \"EndTime\": 1585728674.667851, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 78}, \"StartTime\": 1585728650.812445}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:36 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 79, train loss: 0.0004249664512156558 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:36 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 79, train throughput: 15.33765277 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 79, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 79, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:38 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 79, validation throughput: 43.851758254 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:38 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:11:38 INFO 140322268817216] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1585728698.516724, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 79}, \"StartTime\": 1585728674.668017}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:00 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 80, train loss: 0.00042491813885709763 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:00 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 80, train throughput: 15.1402893519 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 80, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 80, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:02 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 80, validation throughput: 43.3589737436 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:02 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:02 INFO 140322268817216] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 81, \"sum\": 81.0, \"min\": 81}}, \"EndTime\": 1585728722.690732, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 80}, \"StartTime\": 1585728698.516865}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:24 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 81, train loss: 0.000424890023408041 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:24 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 81, train throughput: 15.3975426889 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 81, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 81, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 81, validation throughput: 42.9299379449 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:26 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:26 INFO 140322268817216] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1585728746.475504, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 81}, \"StartTime\": 1585728722.690871}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:48 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 82, train loss: 0.0004248411764580707 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:48 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 82, train throughput: 14.9355626664 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 82, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:50 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 82, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:50 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 82, validation throughput: 43.6648949569 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:50 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:12:50 INFO 140322268817216] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 83, \"sum\": 83.0, \"min\": 83}}, \"EndTime\": 1585728770.625993, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 82}, \"StartTime\": 1585728746.475642}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:12 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 83, train loss: 0.000424894198275931 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:12 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 83, train throughput: 15.3068454602 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 83, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:14 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 83, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:14 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 83, validation throughput: 43.8113597914 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:14 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:14 INFO 140322268817216] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1585728794.412401, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 83}, \"StartTime\": 1585728770.626128}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:36 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 84, train loss: 0.00042493542806416943 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:36 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 84, train throughput: 15.0308682221 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 84, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:38 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 84, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:38 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 84, validation throughput: 43.6173463618 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:38 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:13:38 INFO 140322268817216] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 85, \"sum\": 85.0, \"min\": 85}}, \"EndTime\": 1585728818.640467, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 84}, \"StartTime\": 1585728794.41256}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:00 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 85, train loss: 0.00042491936160092195 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:00 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 85, train throughput: 15.4518387968 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 85, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:02 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 85, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:02 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 85, validation throughput: 43.7882861998 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:02 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:02 INFO 140322268817216] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1585728842.390911, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 85}, \"StartTime\": 1585728818.640624}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:24 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 86, train loss: 0.00042491015751454 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:24 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 86, train throughput: 15.2986444844 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 86, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:26 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 86, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:26 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 86, validation throughput: 43.2455894312 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:26 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:26 INFO 140322268817216] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 87, \"sum\": 87.0, \"min\": 87}}, \"EndTime\": 1585728866.273814, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 86}, \"StartTime\": 1585728842.391052}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:47 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 87, train loss: 0.0004249045718767631 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:47 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 87, train throughput: 15.3203060927 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 87, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 87, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 87, validation throughput: 44.2857147705 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:49 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:14:50 INFO 140322268817216] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1585728890.056183, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 87}, \"StartTime\": 1585728866.273974}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:11 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 88, train loss: 0.00042487399791992316 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:11 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 88, train throughput: 15.3313465616 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 88, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 88, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:13 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 88, validation throughput: 43.0573724888 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:13 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:13 INFO 140322268817216] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 89, \"sum\": 89.0, \"min\": 89}}, \"EndTime\": 1585728913.795323, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 88}, \"StartTime\": 1585728890.056322}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:35 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 89, train loss: 0.00042483766102170765 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:35 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 89, train throughput: 15.0208039459 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 89, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 89, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:37 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 89, validation throughput: 43.9748755676 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:37 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:37 INFO 140322268817216] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1585728937.902897, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 89}, \"StartTime\": 1585728913.795463}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:59 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 90, train loss: 0.0004248605902447659 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:15:59 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 90, train throughput: 15.3508449747 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 90, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 90, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:01 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 90, validation throughput: 43.9949651758 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:01 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:01 INFO 140322268817216] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}}, \"EndTime\": 1585728961.844466, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 90}, \"StartTime\": 1585728937.903034}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:23 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 91, train loss: 0.0004248641872484433 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:23 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 91, train throughput: 15.1919069298 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 91, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 91, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:25 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 91, validation throughput: 43.9618629507 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:25 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:25 INFO 140322268817216] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1585728985.901413, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 91}, \"StartTime\": 1585728961.844663}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:47 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 92, train loss: 0.000424909250317509 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:47 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 92, train throughput: 15.4428022393 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 92, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 92, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 92, validation throughput: 43.4664136072 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:49 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:16:49 INFO 140322268817216] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 93, \"sum\": 93.0, \"min\": 93}}, \"EndTime\": 1585729009.751352, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 92}, \"StartTime\": 1585728985.901556}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:11 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 93, train loss: 0.00042487941850834573 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:11 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 93, train throughput: 15.0950602261 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 93, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 93, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:13 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 93, validation throughput: 43.6077826779 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:13 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:13 INFO 140322268817216] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1585729033.939035, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 93}, \"StartTime\": 1585729009.75152}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:35 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 94, train loss: 0.00042490577509038543 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:35 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 94, train throughput: 15.4180512815 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 94, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 94, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:37 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 94, validation throughput: 43.6244185727 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:37 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:37 INFO 140322268817216] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 95, \"sum\": 95.0, \"min\": 95}}, \"EndTime\": 1585729057.751399, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 94}, \"StartTime\": 1585729033.939176}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:59 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 95, train loss: 0.0004249366033227967 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:17:59 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 95, train throughput: 15.2571373666 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 95, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:01 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 95, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:01 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 95, validation throughput: 43.1831680511 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:01 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:01 INFO 140322268817216] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1585729081.776279, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 95}, \"StartTime\": 1585729057.751586}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:23 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 96, train loss: 0.0004248851607706839 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:23 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 96, train throughput: 14.9309061206 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 96, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:25 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 96, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:25 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 96, validation throughput: 43.8064653421 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:25 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:26 INFO 140322268817216] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 97, \"sum\": 97.0, \"min\": 97}}, \"EndTime\": 1585729106.080666, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 96}, \"StartTime\": 1585729081.776452}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:47 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 97, train loss: 0.00042482589886180664 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:47 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 97, train throughput: 15.3071323063 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 97, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:49 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 97, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:49 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 97, validation throughput: 43.2404703117 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:49 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:18:49 INFO 140322268817216] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1585729129.960304, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 97}, \"StartTime\": 1585729106.080806}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:11 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 98, train loss: 0.0004248994442413708 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:11 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 98, train throughput: 15.3686597314 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 98, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:13 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 98, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:13 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 98, validation throughput: 44.1131717285 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:13 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:13 INFO 140322268817216] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 99, \"sum\": 99.0, \"min\": 99}}, \"EndTime\": 1585729153.521311, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 98}, \"StartTime\": 1585729129.960442}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:35 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 99, train loss: 0.000424873554852399 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:35 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 99, train throughput: 14.9035760679 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 99, validation pixel_accuracy: 1.0 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] #quality_metric. host: algo-1, epoch: 99, validation mIOU: 0.5 .\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] #throughput_metric. host: algo-1, epoch: 99, validation throughput: 43.7898944371 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1585729177.79013, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 99}, \"StartTime\": 1585729153.521449}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 WARNING 140322268817216] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/01/2020 08:19:37 INFO 140322268817216] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"totaltime\": {\"count\": 1, \"max\": 2389511.5201473236, \"sum\": 2389511.5201473236, \"min\": 2389511.5201473236}, \"setuptime\": {\"count\": 1, \"max\": 18.90110969543457, \"sum\": 18.90110969543457, \"min\": 18.90110969543457}}, \"EndTime\": 1585729178.170748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1585726790.015719}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-01 08:19:59 Uploading - Uploading generated training model\n",
      "2020-04-01 08:20:36 Completed - Training job completed\n",
      "Training seconds: 2528\n",
      "Billable seconds: 2528\n"
     ]
    }
   ],
   "source": [
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not. We can choose any sagemaker supported instance we want to host the model. In our case we chose the `ml.p3.2xlarge` instance to train, but we choose to host the model on the less expensive cpu instance, `ml.c5.xlarge`. The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. To do this, let us download an image from [PEXELS](https://www.pexels.com/) which the algorithm has so-far not seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'floorplans/im_105146_22_14_7679_plans_part1_1_page_006.png'\n",
    "filename = PREFIX + '/validation/107.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the image to bytearray before we supply it to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "\n",
    "# resize image size for inference\n",
    "im = PIL.Image.open(filename)\n",
    "im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint accepts images in formats similar to the ones found images in the training dataset. It accepts the `image/jpeg` `content_type`. The `accept` parameter takes on two values: `image/png` and `application/x-protobuf`. For customers who want an indexed-PNG segmentation mask such as the ones that were used during training, can use the `image/png` accept type as shown in the example below. Using this endpoint will return a image bytearray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 ms, sys: 85 µs, total: 11.8 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ss_predictor.content_type = 'image/png'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "num_classes = 2\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second `accept` type allows us to request all the class probabilities for each pixels. Let us use our endpoint to try to predict the probabilites of segments within this image. Since the image is `jpeg`, we use the appropriate `content_type` to run the prediction job. The endpoint returns a file that we can simply load and peek into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 ms, sys: 4 ms, total: 22.9 ms\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# resize image size for inference\n",
    "im = PIL.Image.open(filename)\n",
    "im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)\n",
    "    \n",
    "ss_predictor.content_type = 'image/png'\n",
    "ss_predictor.accept = 'application/x-protobuf'\n",
    "results = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we receive back is a recordio-protobuf of probablities sent as a binary. It takes a little bit of effort to convert into a readable array. Let us convert them to numpy format. We can make use of `mxnet` that has the capability to read recordio-protobuf formats. Using this, we can convert the outcoming bytearray into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.record_pb2 import Record\n",
    "import mxnet as mx\n",
    "\n",
    "results_file = 'results.rec'\n",
    "with open(results_file, 'wb') as f:\n",
    "    f.write(results)\n",
    "\n",
    "rec = Record()\n",
    "recordio = mx.recordio.MXRecordIO(results_file, 'r')\n",
    "protobuf = rec.ParseFromString(recordio.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protobuf array has two parts to it. The first part contains the shape of the output and the second contains the values of probabilites. Using the output shape, we can transform the probabilities into the shape of the image, so that we get a map of values. There typically is a singleton dimension since we are only inferring on one image. We can also remove that using the `squeeze` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(rec.features[\"target\"].float32_tensor.values)\n",
    "shape = list(rec.features[\"shape\"].int32_tensor.values)\n",
    "shape = np.squeeze(shape)\n",
    "mask = np.reshape(np.array(values), shape)\n",
    "mask = np.squeeze(mask, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as to plot the segmentation mask from the list of probabilities, let us get the indices of the most probable class for each pixel. We can do this by measuring the `argmax` across the classes axis of the probability data. To plot the probabilites as image, we can use the `numpy.argmax` method to find out which probabilities are the largest and plot only those as a segmentaiton mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_map = np.argmax(mask, axis=0)\n",
    "num_classes = 2\n",
    "plt.imshow(pred_map, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To call any endpoint use the list endpoint api and boto below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            \"EndpointName\": \"ss-floorplans-demo-estimator-cubicasa8x-2020-04-01-07-35-06-703\",\n",
      "            \"EndpointArn\": \"arn:aws:sagemaker:us-east-2:308537982265:endpoint/ss-floorplans-demo-estimator-cubicasa8x-2020-04-01-07-35-06-703\",\n",
      "            \"EndpointName\": \"ss-floorplans-demo-estimator-cubicasa8x-2020-03-30-07-08-01-723\",\n",
      "            \"EndpointArn\": \"arn:aws:sagemaker:us-east-2:308537982265:endpoint/ss-floorplans-demo-estimator-cubicasa8x-2020-03-30-07-08-01-723\",\n",
      "            \"EndpointName\": \"ss-floorplans-demo-estimator-cubicasa8x-2020-03-29-05-13-10-003\",\n",
      "            \"EndpointArn\": \"arn:aws:sagemaker:us-east-2:308537982265:endpoint/ss-floorplans-demo-estimator-cubicasa8x-2020-03-29-05-13-10-003\",\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker list-endpoints | grep ss-floorplans-demo-estimator-cubicasa8x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sagemaker/semantic-segmentation-cubicasa5k/validation/107.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker')\n",
    "endpoint_name='ss-floorplans-demo-estimator-cubicasa8x-2020-03-29-05-13-10-003'\n",
    "print(filename)\n",
    "with open(filename, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='image/png', \n",
    "                                   Accept='image/png', \n",
    "                                   Body=payload)\n",
    "\n",
    "return_img = response['Body'].read()\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "num_classes = 1\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
